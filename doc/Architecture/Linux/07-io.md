# Disk IO

[IO performance](https://www.cnblogs.com/muahao/p/6596545.html)

[performance](http://www.strolling.cn/2018/07/metric-driven-5-draw-metrics/)

[iowait](https://blog.51cto.com/291268154/1981358)

[linux io performance tunning](https://www.cnblogs.com/sky-heaven/p/9849280.html)

[OS tunning](https://yq.aliyun.com/articles/15239)

[cgroup管理进程磁盘IO](https://blog.csdn.net/sofia1217/article/details/49158493)

[dirty page](http://blog.sina.com.cn/s/blog_448574810101k1va.html)

## 基本概念

### 读写IO(Read/Write)操作

- 写IO
- 读IO

### 单个IO操作
当控制磁盘的控制器接到操作系统的读IO操作指令的时候，控制器就会给磁盘发出一个读数据的指令，并同时将要读取的数据块的地址传递给磁盘，然后磁盘会将读取到的数据传给控制器，并由控制器返回给操作系统，完成一个读IO的操作；同样的，一个写IO的操作也类似，控制器接到写的IO操作的指令和要写入的数据，并将其传递给磁盘，磁盘在数据写入完成之后将操作结果传递回控制器，再由控制器返回给操作系统，完成一个写IO的操作。

### 随机访问与连续访问
随机访问指的是本次IO所给出的扇区地址和上次IO给出扇区地址相差比较大，这样的话磁头在两次IO操作之间需要比较大的移动动作才能重新开始读/写数据。相反的，如果当次IO给出的扇区地址与上次IO结束的扇区地址一致或者是接近的话，那磁头就能很快的开始这次IO操作，这样的多个IO操作称为连续访问。

### 顺序IO模式/并发模式
磁盘控制器可能会一次对磁盘组发出一连串的IO命令，如果磁盘组一次只能执行一个IO命令时称为顺序IO;当磁盘组能同时执行多个IO命令时，称为并发IO。并发IO只能发生在由多个磁盘组成的磁盘组上，单块磁盘只能一次处理一个IO命令。

### 单个IO的大小(IO Chunk Size)

操作系统为了提高IO的性能引入了文件系统缓存(Filesystem Cache), 系统会根据请求的数据的情况将多个来自IO的请求先放在缓存里，然后一次性的提交给磁盘，也就是说对于数据库发出的多个8K数据块的读操作可能放在一个IO里就处理了。

还有对于有些存储系统也是提供了缓存(cache)的，接收到操作系统的IO请求后也是会将多个操作系统的IO请求合并成一个来处理。不管是操作系统层面的缓存还是磁盘控制器层面的缓存，目的只有一个，提高数据读写的效率。因此每次单独的IO操作大小都是不一样的，它主要取决于系统对数据读写效率的判断。

文件系统里面我们也能碰到一个文件系统的块，在现在很多的linux系统中都是4K(通过/usr/bin/time -v能看到)。但是说到单次IO的大小，跟这些块的大小是没有直接关系的，在英文里单次IO大小通常被称为IO chunk size，不会是IO Block size。

### IOPS
IO系统每秒所执行IO操作的次数，是一个重要的用来衡量系统IO能力的一个参数。对于单个磁盘组成的IO系统来说，计算它的IOPS不是一件很难的事情，只要我们知道了系统完成一次IO所需要的时间的话就能推算出系统的IOPS来。

#### 机械硬盘
假设磁盘的转速(Rotational Speed)为15K RPM，平均寻道时间为5ms，最大传输速率40MB/s。

当控制器对磁盘发出一个IO操作命令的时候，磁盘的驱动臂(Actuator Arm)带读写磁头(Head)离开着陆区(Landing Zone，位于内圈没有数据的区域)，移动到要操作的初始数据块所在的磁道(Track)的正上方，这个过程被称为寻址(Seeking)，对应消耗的时间被称为寻址时间；但是找到对应磁道还不能马上读取数据，这时候磁头要等到磁盘盘片(Platter)旋转到初始数据块所在的扇区(Sector)落在读写磁头正上方之后才能开始读取数据，在这个等待盘片旋转的可操作扇区的过程中消耗的时间称为旋转延时(Rotational Time)。完成一次IO操作。

- 寻道时间
    - 3-5ms
- 盘片旋转时间
    - 15000rpm =》2ms
- 最大传输速度
    - 300MB/s

IOPS理论最大IOPS：1000/5=200

#### 固态硬盘
固态硬盘SSD是一种电子装置，避免了传统磁盘在寻道和旋转上的时间花费，存储单元寻址开销大大降低，因此IOPS可以非常高，能够达到数万甚至数十万。实际测量中，IOPS数值会受到很多因素的影响，包括I/O负载特征(读写比例，顺序和随机，工作线程数，队列深度，数据记录大小)、系统配置、操作系统、磁盘驱动等等。因此对比测量磁盘IOPS时，必须在同样的测试基准下进行，即便如何也会产生一定的随机不确定性。通常情况下，IOPS可细分为如下几个指标：

Total IOPS，混合读写和顺序随机I/O负载情况下的磁盘IOPS，这个与实际I/O情况最为相符，大多数应用关注此指标。

- Random Read IOPS
    - 100%随机读负载情况下的IOPS
- Random Write IOPS
    - 100%随机写负载情况下的IOPS
- Sequential Read IOPS
    - 100%顺序读负载情况下的IOPS
- Sequential Write IOPS
    - 100%顺序写负载情况下的IOPS

IOPS的测试benchmark工具主要由Iometer，IoZone， FIO等，可以综合用于测试磁盘在不同情形下的IOPS。对于应用系统，需要首先确定数据的负载特征，然后选择合理的IOPS指标进行测量和对比分析，据此选择合适的存储介质和软件系统。

### 传输速度(Transfer Rate)和吞吐率(Throughput)
#### Performance
- 顺序IO
    - 数据库执行大量的查询、流媒体服务等
    - 关注吞吐能力
- 随机IO
    - 每次请求的数据很小，随机出现
    - IOPS

### IO响应时间(IO Response Time)
IO响应时间，也被称为IO延时(IO Latency)，IO响应时间就是从操作系统内核发出的一个读或写的IO命令道操作系统内核接收到IO回应的时间，包括IO操作在IO等待队列中所花费的等待时间。

IO操作的等待队列，遵循M/M/1模型

随着系统实际IOPS越接近理论的最大值，IO的响应时间会成非线性的增长，越是接近最大值，响应时间变得越大，而且会比预期超出很多。一般来说在实际的应用中有一个70%的指导值，也就是说在IO读写的队列中，当队列大小小于70%的时候，IO的响应时间增加会很小，相对来说让人比较能接受的，一旦超过70%，响应时间就会戏剧性的暴增，所以当以系统的IO压力超出最大可承受压力的70%的时候就是必须要考虑调整或升级了。

70%的指导值也适用于CPU响应时间，这也是在时间中证明过的，一旦CPU超过70%，系统将会变得受不了的慢。

## 内核中和IO相关概念

### IO
磁盘通常是计算机最慢的子系统，也是最容易出现性能瓶颈的地方，因为磁盘距离CPU最远而且CPU访问磁盘要涉及到机械操作，比如转轴、寻轨等。访问硬盘和访问内存之间的速度差别是以数量级来计算的。

内存和硬盘之间的IO是以页为单位来进行的，在Linux系统上页大小默认为4K，使用/usr/bin/time -v date来查看

### 缺页中断
Linux利用虚拟内存极大的扩展了程序地址空间，使得原来物理内存不能容下的程序也可以通过内存和硬盘之间的不断交换(把暂时不用的内存页交换到硬盘，把需要的内存页从硬盘读到内存)来赢得更多的内存，看起来就像物理内存被扩大了一样。事实上这个过程对程序是完全透明的，程序完全不用理会自己哪一部分、什么时候被交换进内存，一切都有内核的虚拟内存管理来完成。当程序启动的时候，Linux内核首先检查CPU的缓存和物理内存，如果数据已经在内存里就忽略，如果数据不再内存里就引起一个缺页中断(Page Fault)，然后从硬盘读取缺页，并把缺页缓存到物理内存里。缺页中断可分为主缺页中断(Major Page Fault)和次缺页中断(Minor Page Fault)，要从磁盘读取数据而产生的中断是主动缺页中断，数据已经被读入内存并缓存起来，从内存缓存区中而不是直接从硬盘中读取数据而产生的中断是次缺页中断。

上面的内存缓存区起到了预读硬盘的作用，内核先在物理内存里寻找缺页，没有的话产生次缺页中断从内存缓存里找，如果还没有发现的话就从硬盘读取。很显然，把多余的内存拿出来做成内存缓存区提高了访问速度，这里还有一个命中率的问题，运气好的话如果每次缺页都能从内存缓存区读取的话将会极大提高性能。要提高命中率的一个简单方法就是增大内存缓存区面积，缓存区越大预存的页面就越多，命中率也会越高。

### File Buffer Cache
内存缓冲区(也叫文件缓存区File Buffer Cache)读取页比从硬盘读取要快得多，所以Linux内核希望能尽可能产生次缺页中断(从文件缓存区读)，并且能尽可能避免主缺页中断(从硬盘里读)，这样随着次缺页中断的增多，文件缓存区也逐步增大，直到系统只有少量可用物理内存的时候Linux才开始释放一些不用的页。运行Linux一段时间后会发现虽然系统上运行的程序不多，但是可用内存总是很少，这样给大家造成了Linux对内存管理很低效的假象，事实上Linux把那些暂时不用的物理内存高效地利用起来做预存(内存缓存区)。

    cat /proc/meminfo

- Memory Total, 总物理内存
- MemFree，可用内存
- Buffers，磁盘缓存
- Cached，文件缓存区

Linux中内存页面有三种类型：
- Read Page只读页(或代码页)
    - 那些通过主缺页中断从硬盘读取的页面，包括不能修改的静态文件、可执行程序、库文件等。
- Dirty Page脏页
    - 指那些在内存中被修改过的数据页，比如文本文件等。这些文件由pdflush负责同步到硬盘，内存不足的时候由kswapd和pdflush把数据写回硬盘并释放内存
- Anonymous pages匿名页
    - 那些属于某个进程但是又和任何文件无关联，不能被同步到硬盘上，内存不足的时候由kswapd负责将他们写到交换分区并释放内存

### 相关参数

/proc/sys/vm/

- dirty_background_ratio
    - dirty page达到系统整体内存的百分比时，触发pdflush进程把dirty page写回磁盘
- dirty_expire_centisecs
    - 0.01 * dirty_expire_centisecs (second)
    - dirty page内存驻留超时时间
    - pdflush在下一次将写回磁盘
- dirty_ratio
    - 进程产生的脏数据达到系统比例，进程自行将数据写回磁盘
- dirty_writeback_centisecs
    - 0.01 * dirty_expire_centisecs (second)
    - pdflush进程写回周期

### 触发机制

- 定时方式
    - 只会writeback超时的dirty page
- 内存不足
    每次writeback 1024个page，直到满足要求
- dirty page超过一定比例
    - 超过dirty_background_ratio，write调用pdflush写回；超过dirty_ratio，阻塞进程write，主动写回dirty page
    - 直到dirty page恢复到正常比例

### 大数据量数据写入磁盘
1. 如果写入量巨大，不能期待系统缓存的自动回写机制，最好采用应用层调用fsync或sync。如果写入量大，甚至超过了系统缓存自动写回的速度，就有可能导致系统的dirty page率超过dirty_ratio，这个时候系统就会阻断后续的操作，这个阻塞有可能有5分钟之久。因此，一种建议的方式是在应用层，在合适的时机调用fsync。
2. 对于关键性能，最好不要依赖系统cache的作用，如果对性能要求比较高，最好在自己的应用层实现自己的cache，因为系统cache受外界影响太大，说不定什么时候，系统cache就被冲走了

### Scenarios

**1. Decreasing the cache**
In many cases we have fast disk subsystems with their own big, battery-backed NVRAM caches, so keeping things in the OS page cache is risky

To do this we lower vm.dirty_backgroud_ratio and vm.dirty_ratio in /etc/sysctl.conf and reloading with sysctl -p

This is a typical approach on virtual machines, as well as Linux-based hypervisor.

**2. Increasing the cache**
Thers are scenarios where raising the cache dramatically has positive effects on performance. These situations are where the data contained on a linux guest isn't critical and can be lost, and usually where an application is writing to the same files repeatly or in repeatable bursts.

**3. Both ways**
There are also scenarios where a system has to deal with infrequent, bursty traffic to slow disk(batch jobs at the top of the hour, midnight, writing to an SD card on the Rasperry Pi, etc.)

### cgroup管理进程磁盘IO

#### blk子系统可修改参数

- blkio.throttle.read_bps_device
- blkio.throttle.read_iops_device
- blkio.throttle.write_bps_device
- blkio.throttle.write_iops_device
- blkio.weight
- blkio.weight_device

**throttle vs weight**
类似cpu子系统的quota和shares，即绝对限制vs相对限制；相对限制可以保证在不繁忙的时候充分利用资源

#### blk子系统报告参数(只读)

- blkio.time
- blkio.sector
- blkio.io_service_bytes
- blkio.io_serviced
- blkio.io_service_time
- blkio.io_wait_time
- blkio.io_merged
- blkio.io_queued
